# -*- coding: utf-8 -*-
"""FAQs based elearning chatbot - V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w6Dw4kVWu8pj-7tYUcgjWQ4AmkZyrYIO
"""

import os

# Commented out IPython magic to ensure Python compatibility.
# I took help from this link: https://python.langchain.com/docs/integrations/chat/google_generative_ai/
HF_TOKEN = os.getenv('HF_TOKEN')


#import getpass

if "GOOGLE_API_KEY" not in os.environ:
    print("not there at first")
    os.environ["GOOGLE_API_KEY"] = os.getenv('GOOGLE_API_KEY')
    #os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")


# %pip install -qU langchain-google-genai

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.2,
    max_output_tokens=8192,
    timeout=None,
    max_retries=2,
    top_p= None,   # you can try 0.95
    top_k= None,    # you can try 64
    # other params...
    # https://api.python.langchain.com/en/latest/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html
)

'''messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]'''

messages = [
    ("human", "Where is America located?"),
]


# Invoke the LLM with only the human message
ai_msg = llm.invoke(messages)

# Output the response
print(ai_msg.content)

"""America is a broad term that can refer to:

* **The United States of America:** Located in **North America**, bordered by Canada to the north and Mexico to the south.
* **The Americas:**  A collective term for the continents of **North America**, **Central America**, and **South America**.

So, depending on what you mean by "America," the answer is either **North America** or **North, Central, and South America**.
"""

ai_msg.usage_metadata

!pip install langchain
!pip install -U langchain-community

from langchain.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path='faqs.csv', encoding="ISO-8859-1", source_column="prompt")
# I have changed the encoding to resolve the error

# Store the loaded data in the 'data' variable
data = loader.load()

# Commented out IPython magic to ensure Python compatibility.
#%pip install sentence-transformers
!pip install --upgrade sentence-transformers==2.2.2
# To use the HuggingFaceInstructEmbeddings we need to change the version of sentence transformers to a lower one.
# %pip install InstructorEmbedding

#!pip install --upgrade huggingface-hub transformers

from langchain.embeddings import HuggingFaceInstructEmbeddings

instructor_embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-large")

from langchain.embeddings import HuggingFaceInstructEmbeddings

model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}

# Adjust the initialization to remove the 'token' argument
hf = HuggingFaceInstructEmbeddings(
    model_name="hkunlp/instructor-large",
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# Use the embedding model as needed
e = hf.embed_query("What is your refund policy?")
len(e)
# Model captures the meaning of "What is your refund policy"

!pip install faiss-cpu

from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

# Create a FAISS instance for vector database from 'data'
vectordb = FAISS.from_documents(documents=data,
                                 embedding=hf)

# Create a retriever for querying the vector database
retriever = vectordb.as_retriever(score_threshold = 0.7)


# This cell took a lot of time, approx 4mins

rdocs = retriever.invoke("how about job placement support?")   # I've changed the method from get_relevant_documents
rdocs

from langchain.prompts import PromptTemplate

prompt_template = """Given the following context and a question, generate an answer based on this context only.
In the answer try to provide as much text as possible from "response" section in the source document context without making much changes.
If the answer is not found in the context, kindly state "I don't know." Don't try to make up an answer.

CONTEXT: {context}

QUESTION: {question}"""


PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]  # focus here
)
chain_type_kwargs = {"prompt": PROMPT}


from langchain.chains import RetrievalQA

chain = RetrievalQA.from_chain_type(llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            input_key="query",
                            return_source_documents=True,
                            chain_type_kwargs=chain_type_kwargs)

chain.invoke('Do you provide job assistance and also do you provide job gurantee?')['result']

'''The above response is a merger of two responses, but the model has removed this line:
 "The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job."
'''
pass

chain.invoke("Do you guys provide internship and also do you offer EMI payments?")

chain.invoke("Do you guys provide internship and also do you offer EMI payments?")['result']

chain.invoke("Do you guys provide internship and also do you offer EMI payments?")['result']



